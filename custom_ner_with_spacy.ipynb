{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMGGlWQ/WHU94BqzCeZt6/3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bxsajid/BioBertProject/blob/master/custom_ner_with_spacy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pXZ2qxZsr2pX"
      },
      "source": [
        "import random\n",
        "import time\n",
        "from itertools import chain\n",
        "from os import path, mkdir\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import spacy\n",
        "from matplotlib.ticker import MaxNLocator\n",
        "from spacy import displacy\n",
        "from spacy.util import minibatch, compounding"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r_xq4Db_r8rx"
      },
      "source": [
        "if not path.isdir('data/'):\n",
        "    mkdir('data/')\n",
        "if not path.isdir('models/'):\n",
        "    mkdir('models/')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sjRbHyJZsBQ7"
      },
      "source": [
        "def load_data_spacy(file_path):\n",
        "    \"\"\" Converts data from:\n",
        "    label \\t word \\n label \\t word \\n \\n label \\t word\n",
        "    to: sentence, {entities : [(start, end, label), (start, end, label)]}\n",
        "    \"\"\"\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        file = f.readlines()\n",
        "\n",
        "    training_data, entities, sentence, unique_labels = [], [], [], []\n",
        "    start, end = 0, 0  # initialize counter to keep track of start and end characters\n",
        "\n",
        "    for line in file:\n",
        "        line = line.strip('\\n').split('\\t')\n",
        "        # lines with len > 1 are words\n",
        "        if len(line) > 1:\n",
        "            label = line[1]\n",
        "            label_type = label[0]  # beginning of annotations - \"B-xxx\", intermediate - \"I-xxx\"\n",
        "\n",
        "            word = line[0]\n",
        "            sentence.append(word)\n",
        "            start = end\n",
        "            end += (len(word) + 1)  # length of the word + trailing space\n",
        "\n",
        "            if label_type == 'I':  # if at the end of an annotation\n",
        "                entities.append((start, end - 1, label))  # append the annotation\n",
        "            if label_type == 'B':  # if beginning new annotation\n",
        "                entities.append((start, end - 1, label))  # start annotation at beginning of word\n",
        "\n",
        "            if label != 'O' and label not in unique_labels:\n",
        "                unique_labels.append(label)\n",
        "\n",
        "        # lines with len == 1 are breaks between sentences\n",
        "        if len(line) == 1:\n",
        "            if len(entities) > 0:\n",
        "                sentence = ' '.join(sentence)\n",
        "                training_data.append([sentence, {'entities': entities}])\n",
        "            # reset the counters and temporary lists\n",
        "            start, end = 0, 0\n",
        "            entities, sentence = [], []\n",
        "\n",
        "    return training_data, unique_labels"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cH3v8Qa3sEmO"
      },
      "source": [
        "def calc_precision(pred, true):\n",
        "    precision = len([x for x in pred if x in true]) / (len(pred) + 1e-20)  # true positives / total pred\n",
        "    return precision\n",
        "\n",
        "\n",
        "def calc_recall(pred, true):\n",
        "    recall = len([x for x in true if x in pred]) / (len(true) + 1e-20)  # true positives / total test\n",
        "    return recall\n",
        "\n",
        "\n",
        "def calc_f1(precision, recall):\n",
        "    f1 = 2 * ((precision * recall) / (precision + recall + 1e-20))\n",
        "    return f1"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fa68gWLosICw"
      },
      "source": [
        "# run the predictions on each sentence in the test dataset, and return the spacy object\n",
        "def evaluate(ner, data):\n",
        "    preds = [ner(x[0]) for x in data]\n",
        "\n",
        "    precisions, recalls, f1s = [], [], []\n",
        "\n",
        "    # iterate over predictions and test data and calculate precision, recall, and F1-score\n",
        "    for pred, true in zip(preds, data):\n",
        "        true = [x[2] for x in list(chain.from_iterable(true[1].values()))]  # x[2] = annotation, true[1] = (start, end, annot)\n",
        "        pred = [i.label_ for i in pred.ents]  # i.label_ = annotation label, pred.ents = list of annotations\n",
        "\n",
        "        precision = calc_precision(true, pred)\n",
        "        precisions.append(precision)\n",
        "        recall = calc_recall(true, pred)\n",
        "        recalls.append(recall)\n",
        "        f1s.append(calc_f1(precision, recall))\n",
        "\n",
        "    # print('Precision: {}\\nRecall: {}\\nF1-score: {}'.format(np.around(np.mean(precisions), 3),\n",
        "    #                                                        np.around(np.mean(recalls), 3),\n",
        "    #                                                        np.around(np.mean(f1s), 3)))\n",
        "\n",
        "    return {\n",
        "        'textcat_p': np.mean(precisions),\n",
        "        'textcat_r': np.mean(recalls),\n",
        "        'textcat_f': np.mean(f1s)\n",
        "    }"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L3hy6yAgsLhh"
      },
      "source": [
        "def train_spacy(train_data, labels, iterations, dropout=0.2, display_freq=1):\n",
        "    \"\"\" Train a spacy NER model, which can be queried against with test data\n",
        "\n",
        "    train_data : training data in the format of (sentence, {entities: [(start, end, label)]})\n",
        "    labels : a list of unique annotations\n",
        "    iterations : number of training iterations\n",
        "    dropout : dropout proportion for training\n",
        "    display_freq : number of epochs between logging losses to console\n",
        "    \"\"\"\n",
        "    valid_f1scores, test_f1scores = [], []\n",
        "    nlp = spacy.load('en_core_web_sm')\n",
        "    # nlp = spacy.load('en')\n",
        "    if 'ner' not in nlp.pipe_names:\n",
        "        ner = nlp.create_pipe('ner')\n",
        "        nlp.add_pipe(ner)\n",
        "    else:\n",
        "        ner = nlp.get_pipe('ner')\n",
        "\n",
        "    # Add entity labels to the NER pipeline\n",
        "    for i in labels:\n",
        "        ner.add_label(i)\n",
        "\n",
        "    # Disable other pipelines in SpaCy to only train NER\n",
        "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n",
        "    with nlp.disable_pipes(*other_pipes):\n",
        "        # nlp.vocab.vectors.name = 'spacy_model'  # without this, spaCy throws an \"unnamed\" error\n",
        "        optimizer = nlp.begin_training()\n",
        "        for itr in range(iterations):\n",
        "            random.shuffle(train_data)  # shuffle the training data before each iteration\n",
        "            losses = {}\n",
        "            batches = minibatch(train_data, size=compounding(16., 64., 1.5))\n",
        "            for batch in batches:\n",
        "                texts, annotations = zip(*batch)\n",
        "                nlp.update(\n",
        "                    texts,\n",
        "                    annotations,\n",
        "                    drop=dropout,\n",
        "                    sgd=optimizer,\n",
        "                    losses=losses)\n",
        "            # if itr % display_freq == 0:\n",
        "            #     print(\"Iteration {} Loss: {}\".format(itr + 1, losses))\n",
        "\n",
        "            print('\\n========================================')\n",
        "            print(f'Interaction = {str(itr)}')\n",
        "            print(f'Losses = {str(losses)}')\n",
        "\n",
        "            scores = evaluate(nlp, VALID_DATA)\n",
        "            valid_f1scores.append(scores[\"textcat_f\"])\n",
        "            print('========= VALID DATA ====================')\n",
        "            print(f'F1-score = {str(scores[\"textcat_f\"])}')\n",
        "            print(f'Precision = {str(scores[\"textcat_p\"])}')\n",
        "            print(f'Recall = {str(scores[\"textcat_r\"])}')\n",
        "\n",
        "            scores = evaluate(nlp, TEST_DATA)\n",
        "            test_f1scores.append(scores[\"textcat_f\"])\n",
        "            print('========= TEST DATA =====================')\n",
        "            print(f'F1-score = {str(scores[\"textcat_f\"])}')\n",
        "            print(f'Precision = {str(scores[\"textcat_p\"])}')\n",
        "            print(f'Recall = {str(scores[\"textcat_r\"])}')\n",
        "\n",
        "    return nlp, valid_f1scores, test_f1scores"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZaL9w4RBsRov"
      },
      "source": [
        "def load_model(model_path):\n",
        "    \"\"\" Loads a pre-trained model for prediction on new test sentences\n",
        "\n",
        "    model_path : directory of model saved by spacy.to_disk\n",
        "    \"\"\"\n",
        "    nlp = spacy.blank('en')\n",
        "    if 'ner' not in nlp.pipe_names:\n",
        "        ner = nlp.create_pipe('ner')\n",
        "        nlp.add_pipe(ner)\n",
        "\n",
        "    ner = nlp.from_disk(model_path)\n",
        "    return ner"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TwjjkOuHsbNE"
      },
      "source": [
        "TRAIN_DATA, LABELS = load_data_spacy('data/train.tsv')\n",
        "TEST_DATA, _ = load_data_spacy('data/test.tsv')\n",
        "VALID_DATA, _ = load_data_spacy('data/train_dev.tsv')"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6zZ2dkTjstzo",
        "outputId": "da8815e9-d033-4cbb-df7f-24b6198924d5"
      },
      "source": [
        "# Train (and save) the NER model\n",
        "ner, valid_f1scores, test_f1scores = train_spacy(TRAIN_DATA, LABELS, 20)\n",
        "ner.to_disk('models/spacy_example')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "========================================\n",
            "Interaction = 0\n",
            "Losses = {'ner': 167383.75520324707}\n",
            "========= VALID DATA ====================\n",
            "F1-score = 0.8794585951816769\n",
            "Precision = 0.8617684085645769\n",
            "Recall = 0.9171783898356788\n",
            "========= TEST DATA =====================\n",
            "F1-score = 0.8812412051670144\n",
            "Precision = 0.8661500138059368\n",
            "Recall = 0.9169240403322956\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UN7HBGy9sveI"
      },
      "source": [
        "x = range(20)\n",
        "ax = plt.figure().gca()\n",
        "ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
        "ax.plot(valid_f1scores, label='Validation F1-score')\n",
        "ax.plot(test_f1scores, label='Test F1-score')\n",
        "ax.set_xlabel('Iterations')\n",
        "ax.set_ylabel('F1-score')\n",
        "ax.legend()\n",
        "ax.set_title('F1-score vs iterations for validation and test data')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pIv3fORds1xu"
      },
      "source": [
        "# Let's test our model on test data\n",
        "ner = load_model('models/spacy_example')\n",
        "\n",
        "test_sentences = [x[0] for x in TEST_DATA[:4000]]  # extract the sentences from [sentence, entity]\n",
        "for test_sentence in test_sentences:\n",
        "    doc = ner(test_sentence)\n",
        "    for ent in doc.ents:\n",
        "        print(ent.text, ent.start, ent.char, ent.end, ent.label)\n",
        "    displacy.render(doc, jupyter=True, style='ent')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}